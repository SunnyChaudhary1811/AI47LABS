{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify the URL of the website you want to scrape\n",
    "url = \"https://www.newsweek.com/worlds-best-hospitals-2022\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "def hospital_list():\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        hospital_links = []\n",
    "        rows = table.find_all('tr')[1:]\n",
    "        for row in rows:\n",
    "            link = row.find('a')['href']\n",
    "            hospital_links.append(link)\n",
    "        hospital_links = hospital_links[:50] #can be changed from 50\n",
    "        print(f\"Successfully scraped top {len(hospital_links)} hospital links\")\n",
    "        # Print the links\n",
    "        # for link in hospital_links:\n",
    "        #     print(link)\n",
    "    else:\n",
    "        print(f\"Error {response.status_code} occurred.\")\n",
    "    return hospital_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped top 50 hospital links\n",
      "scrapping 0. https://www.mayoclinic.org/patient-visitor-guide/minnesota\n",
      "scrapping 1. http://www.clevelandclinic.org\n",
      "scrapping 2. https://www.massgeneral.org/\n",
      "scrapping 3. http://www.uhn.ca\n",
      "scrapping 4. http://www.charite.de\n",
      "scrapping 5. https://www.hopkinsmedicine.org/the_johns_hopkins_hospital\n",
      "scrapping 6. http://www.pitiesalpetriere.aphp.fr/\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 7. https://www.karolinska.se/\n",
      "scrapping 8. https://www.uclahealth.org/reagan/\n",
      "scrapping 9. https://eng.sheba.co.il/\n",
      "scrapping 10. https://www.chuv.ch/fr/chuv-home/\n",
      "scrapping 11. http://www.sgh.com.sg/\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 12. http://www.h.u-tokyo.ac.jp/\n",
      "scrapping 13. https://www.unispital-basel.ch\n",
      "scrapping 14. http://www.usz.ch/Seiten/default.aspx\n",
      "scrapping 15. http://www.klinikum.uni-heidelberg.de\n",
      "scrapping 16. https://www.brighamandwomens.org/\n",
      "scrapping 17. http://www.hopital-georgespompidou.aphp.fr/\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 18. https://www.en.auh.dk/about-the-hospital/organisation/\n",
      "scrapping 19. http://www.stanfordhealthcare.org\n",
      "scrapping 20. https://www.nyp.org/\n",
      "scrapping 21. https://www.mri.tum.de/\n",
      "scrapping 22. http://hospital.luke.ac.jp\n",
      "scrapping 23. http://www.akhwien.at\n",
      "scrapping 24. https://www.sunnybrook.ca/\n",
      "scrapping 25. https://www.rigshospitalet.dk/\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 26. https://www.mountsinai.on.ca/\n",
      "scrapping 27. https://www.nm.org/\n",
      "scrapping 28. https://www.mountsinai.org/locations/mount-sinai\n",
      "scrapping 29. http://eng.amc.seoul.kr/gb/lang/main.do\n",
      "scrapping 30. https://www.oslo-universitetssykehus.no/\n",
      "scrapping 31. https://www.hug-ge.ch\n",
      "scrapping 32. https://www.mhh.de/\n",
      "scrapping 33. https://www.einstein.br/Pages/Home.aspx\n",
      "scrapping 34. https://www.lmu-klinikum.de/\n",
      "scrapping 35. http://www.med.umich.edu/\n",
      "scrapping 36. https://www.policlinicogemelli.it/\n",
      "scrapping 37. //www.amsterdamumc.org/en.htm\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 38. https://www.cedars-sinai.org/\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 39. http://www.dukehealth.org\n",
      "scrapping 40. https://www.guysandstthomas.nhs.uk\n",
      "scrapping 41. https://www.hus.fi/en/Pages/default.aspx\n",
      "scrapping 42. http://www.samsunghospital.com/home/main/index.do\n",
      "scrapping 43. https://www.umcutrecht.nl/nl/\n",
      "scrapping 44. https://www.uzleuven.be/nl\n",
      "scrapping 45. http://www.kameda.com/ja/general/index.html\n",
      "scrapping 46. http://www.chru-lille.fr\n",
      "error occured at {i} {website_url}\n",
      "Saved Backup Json File!!!!!!\n",
      "scrapping 47. http://www.ucsfhealth.org\n",
      "scrapping 48. http://www.uke.de\n",
      "scrapping 49. https://www.ospedaleniguarda.it/\n",
      "Saved Json File!!!!!!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file will scrap the website that we will get from topHospital file.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "# from topHospital import hospital_list\n",
    "\n",
    "MAX_DEPTH = 0  # Maximum depth of recursive scraping\n",
    "\n",
    "\n",
    "def scrape_website(url, depth=0, visited_urls=None):\n",
    "    if visited_urls is None:\n",
    "        visited_urls = set()\n",
    "    if depth > MAX_DEPTH or url in visited_urls:\n",
    "        return {}\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        # session = requests.Session()\n",
    "        # session.headers = {\n",
    "        #     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.1.2222.33 Safari/537.36\",\n",
    "        #     \"Accept-Encoding\": \"*\",\n",
    "        #     \"Connection\": \"keep-alive\"\n",
    "        # }\n",
    "        # response = session.get(url)\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            tags_to_scrape = ['p']  # more tags \"div\" & \"span\"\n",
    "            scraped_data = {\n",
    "                \"url\": url,\n",
    "                \"content\": {}\n",
    "            }\n",
    "\n",
    "            for tag_name in tags_to_scrape:\n",
    "                tag_elements = soup.find_all(tag_name)\n",
    "                tag_texts = [element.get_text(strip=True)\n",
    "                             for element in tag_elements]\n",
    "                if tag_texts:\n",
    "                    scraped_data[\"content\"][tag_name] = tag_texts\n",
    "            link_elements = soup.find_all('a', href=True)\n",
    "            links_to_visit = [urljoin(url, element['href'])\n",
    "                              for element in link_elements]\n",
    "\n",
    "            for link in links_to_visit:\n",
    "                link_data = scrape_website(link, depth + 1, visited_urls)\n",
    "                if link_data:\n",
    "                    # Merge the scraped data from the current link into the main dictionary\n",
    "                    for tag_name, tag_texts in link_data[\"content\"].items():\n",
    "                        scraped_data[\"content\"].setdefault(\n",
    "                            tag_name, []).extend(tag_texts)\n",
    "\n",
    "            return scraped_data\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Error {response.status_code} occurred.\")\n",
    "\n",
    "    except requests.exceptions.InvalidSchema:\n",
    "        pass\n",
    "        # print(f\"Invalid URL: {url}\")\n",
    "\n",
    "\n",
    "website_urls = hospital_list()\n",
    "hospitalData = []\n",
    "\n",
    "for i, website_url in enumerate(website_urls):\n",
    "    try:\n",
    "        # Scrape the website\n",
    "        print(f'scrapping {i}. {website_url}')\n",
    "        data = scrape_website(website_url)\n",
    "        hospitalData.append(data)\n",
    "    except:\n",
    "        print('error occured at {i} {website_url}')\n",
    "        with open(f\"backup_scraped_data.json\", \"w\") as file:\n",
    "            json.dump(hospitalData, file, indent=4)\n",
    "            print('Saved Backup Json File!!!!!!')\n",
    "\n",
    "# Save the formatted data to a JSON file\n",
    "with open(\"scraped_data.json\", \"w\") as file:\n",
    "    json.dump(hospitalData, file, indent=4)\n",
    "    print('Saved Json File!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
